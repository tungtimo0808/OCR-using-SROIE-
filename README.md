# ðŸ§¾ Receipt Information Extraction API (Flask + Deep Learning)

This project provides a Flask web application that performs end-to-end receipt information extraction from images.

The system combines:

Text region detection (object detection)

Text recognition (OCR)

Information extraction (NER / field classification)

It returns structured fields such as:

company

date

address

total

# Features

âœ… Upload receipt images via web or API
âœ… Automatic text detection and OCR
âœ… Structured information extraction
âœ… Lazy model loading (load once, reuse)
âœ… Fallback regex for date and total
âœ… Debug metadata output
âœ… GPU supported (optional)

# Pipeline Overview

Input Image
     â†“
[Detection Model]
  Faster R-CNN
  â†’ find text regions
     â†“
[Recognition Model]
  TrOCR
  â†’ convert image crops to text
     â†“
[Information Extraction Model]
  LayoutLMv3
  â†’ classify words into fields
     â†“
Structured JSON Output


# Models Used

| Task                   | Model                                |
| ---------------------- | ------------------------------------ |
| Detection              | Faster R-CNN ResNet50 FPN            |
| Recognition            | TrOCR (microsoft/trocr-base-printed) |
| Information Extraction | LayoutLMv3 (token classification)    |


## Task 1 â€“ Text Detection

We evaluate the detection model using Precision (P), Recall (R), and F1-score at two IoU thresholds (0.5 and 0.7).

Validation Set

| IoU | Precision | Recall | F1     |
| --- | --------- | ------ | ------ |
| 0.5 | 0.9501    | 0.9623 | 0.9561 |
| 0.7 | 0.8974    | 0.9090 | 0.9032 |

Test Set
| IoU | Precision | Recall | F1     |
| --- | --------- | ------ | ------ |
| 0.5 | 0.9488    | 0.9641 | 0.9564 |
| 0.7 | 0.9012    | 0.9158 | 0.9085 |

Analysis

At IoU@0.5, the detector achieves F1 â‰ˆ 0.956 on both VAL and TEST.

At stricter IoU@0.7, performance slightly decreases but remains strong (F1 â‰ˆ 0.90).

The small gap between VAL and TEST indicates good generalization.

Recall is consistently slightly higher than precision, meaning:

The model prefers detecting more boxes.

It rarely misses text regions.

Performance stability across datasets suggests the detector is robust and well-trained.

Overall, the detection component is strong and unlikely to be the bottleneck in the pipeline.

## Task 2 â€“ Text Recognition (OCR)

Recognition performance is measured using:

CER (Character Error Rate)

WER (Word Error Rate)

Exact Accuracy

Validation Set

CER = 0.02095
WER = 0.08947
Exact Accuracy = 0.8525

Test Set

CER = 0.02201
WER = 0.09255
Exact Accuracy = 0.86188

Analysis

CER â‰ˆ 2.2% on TEST â†’ extremely low character-level error.

WER â‰ˆ 9.3%, meaning most words are recognized correctly.

Exact match accuracy â‰ˆ 86%, which is strong for receipt OCR.

Key observations:

Very small gap between VAL and TEST â†’ no overfitting.

Low CER shows the TrOCR fine-tuning worked effectively.

Errors are likely caused by:

Very small fonts

Blurry regions

Special characters or receipt noise

The OCR stage is highly accurate and reliable.

## Task 3 â€“ Information Extraction (SROIE-style Field Evaluation)

Evaluation is done using field-level exact match micro metrics on the TEST set.

TP = 1156
FP = 120
FN = 232
Precision = 0.9060
Recall = 0.8329
F1 = 0.8679
Analysis

Precision â‰ˆ 90.6%

Recall â‰ˆ 83.3%

F1 â‰ˆ 86.8%

This means:

When the model predicts a field â†’ it is usually correct (high precision).

Some fields are still missed (lower recall).

The main source of FN likely comes from:

Missing OCR words

Incorrect token alignment

LayoutLM classification boundary errors

Compared to OCR, the IE stage is currently the most challenging part of the pipeline.


# Project Structure
project/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ uploads/
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ best_det.pt
â”œâ”€â”€ best_rec.pt
â””â”€â”€ layoutlmv3_sroie_out/

# How to run 

pip install -r requirements.txt

python app.py 

http://localhost:5000

using python 3.12.12